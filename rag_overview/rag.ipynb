{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Overview\n",
    "\n",
    "Simple overview of RAG (Retrieval Augmented Generation) and how it works.\n",
    "\n",
    "By the end we will have created a simple RAG system that can answer questions about methane emissions.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- What is RAG and why?\n",
    "- Building an Expert System\n",
    "- Vectors\n",
    "- Vector Databases\n",
    "- Vector Search\n",
    "- RAG System\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is RAG?\n",
    "\n",
    "Rag is Retrival Augmented Generation. The idea is you give an LLM references to answer a question, rather then using the LLMs internal knowledge.\n",
    "\n",
    "To explain why and how lets walk through a simple example. Lets build an LLM to answser questiosn about methane.\n",
    "\n",
    "We will use the openai api to create a methane expert.\n",
    "\n",
    "First, here is one with no context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# The API key is added to the environment variable in .env\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are methane expert, for answering methane related questions. Keep answers short. Make answer markdown.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is AVO?\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "display(Markdown(completion.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer does not give 'audio, visual, and olfactory', instead it makes something else up.\n",
    "\n",
    "This is the main thing RAG tries to prevent.\n",
    "\n",
    "Now we will add the highwood glossary to the RAG prompt, and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file glossary.md and set as a variable called glossary\n",
    "\n",
    "glossary = open(\"../glossary.md\", \"r\").read()\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are a methane expert, for answering methane related questions. Keep answers short. Make answer markdown.\n",
    "\n",
    "Use only the provided references to answer the questions.\n",
    "\n",
    "<References>\n",
    "<Glossary>\n",
    "{glossary}\n",
    "</Glossary>\n",
    "</References>\n",
    "\"\"\"\n",
    "\n",
    "# Create a client object\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is AVO?\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(Markdown(completion.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the LLM gives an accurate answer. This is pretty much what RAG does. Find references to answer the question, give the LLM the references and tell the LLM to answer the question based on the references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Catch\n",
    "So the RAG once you have references is easy! The hard part is getting the right references.\n",
    "\n",
    "LMMs have some limitations which make this task difficult.\n",
    "\n",
    "### Tokens\n",
    "Lets briefly talk about tokens, as understading them is key to understanding the problem.\n",
    "\n",
    "Tokens are how an LLM will break up text to process. A helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken # tiktoken is a python libary for generating tokens from text\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "print(\"Hello,\", enc.encode(\"Hello,\"))\n",
    "print(\"how\", enc.encode(\"how\"))\n",
    "print(\"are\", enc.encode(\"are\"))\n",
    "print(\"you?\", enc.encode(\"you?\"))\n",
    "print(\"Hello, how are you?\", enc.encode(\"Hello, how are you?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Hello, how are you?' translates into 6 tokens. You'll notice that the tokens are different depending if its the words, or the full sentence. This has to do with the semantic meaning of the words in the context of the sentence rather then the words themselves. We won't get into this, but its important to know that the tokens are numberical representations of the semantic meaning of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main thing we need to overcome is the limited number of tokens you can send to an LLM prompt. With GPT-4o the limit is 128K tokens. Thats a lot, but for example, subpart-w documents are > 300k tokens. So unless you have a small set of documents you can use, you need to make the documents that get fed to the LLM dynamic.\n",
    "\n",
    "Methane expert has more then 10 million tokens, so you can't just load them into the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Engine\n",
    "The R in RAG is 'retrival'. This is a search engine.\n",
    "\n",
    "This can be described as the following function.\n",
    "`search_engine(question) -> documents[]`\n",
    "\n",
    "This search engine can really be anything. A websearch, a text search, a database query. All it needs to do is find a set of documents that are relevant to the question.\n",
    "\n",
    "Modern LLM technology have given us a new and powerful way to do these searches. We can use a vector database to store the documents and then use a vector similarity algorithm to find the most relevant documents. What this does is search for things that are **semantically similar** to the question, not just looking for text that matches. Most RAG systems will use a vector sematic search (including methane expert) so lets look into that.\n",
    "\n",
    "A vector is a, well, vector reprenation of text. This is the same way that LLM store the text to do their fancy auto complete. The good news is that openAI has an API to create vector for you from text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.embeddings.create(\n",
    "    input=\"Here is some text!\",\n",
    "    model=\"text-embedding-3-small\",\n",
    "    dimensions=10\n",
    ")\n",
    "\n",
    "print(response.data[0].embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the vector has 10 demensions for to show some clarity. In practice, we use ~1000 demensions.\n",
    "\n",
    "Once text is vectorized, its simply a matter of finding the closest vectors to each other. There are many ways to do this, but if there were only 2 demensions, we could use a simple euclidean distance d = sqrt((x1-x2)^2 + (y1-y2)^2).\n",
    "\n",
    "Here we will use a more sophisticated method called the cosine similarity. The cosine similarity is a measure of how similar two vectors are. It is defined as:\n",
    "\n",
    "cosine_similarity = dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "The dot product is the sum of the products of the corresponding elements of the two vectors. The magnitudes are the square roots of the sum of the squares of the elements of the vectors.\n",
    "\n",
    "The cosine similarity is a value between -1 and 1, where 1 means the vectors are identical and -1 means they are completely dissimilar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(a_emb, b_emb):\n",
    "    a = np.array(a_emb.embedding)\n",
    "    b = np.array(b_emb.embedding)\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "text_1 = \"The dark feline purrs softly.\"\n",
    "text_2 = \"The black cat hums gently.\"\n",
    "text_3 = \"The golden cat purrs loudly.\"\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    input=[text_1, text_2, text_3],\n",
    "    model=\"text-embedding-3-small\",\n",
    ")\n",
    "\n",
    "[v_1, v_2, v_3] = response.data\n",
    "\n",
    "s_1_2 = cosine_similarity(v_1, v_2)\n",
    "s_1_3 = cosine_similarity(v_1, v_3)\n",
    "s_2_3 = cosine_similarity(v_2, v_3)\n",
    "\n",
    "print(f\"d_1_2 = {s_1_2}\")\n",
    "print(f\"d_1_3 = {s_1_3}\")\n",
    "print(f\"d_2_3 = {s_2_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 is the highest similarity score, so you can see that that the first 2 sentences are the most similar although they don't share the same words but the meanings are the same.\n",
    "\n",
    "This is the advantage of the LLM vectors. Similar vectors mean the meanings are similar!\n",
    "\n",
    "So this is all fine for an example, but how do you accomplish this at scale?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Database\n",
    "A specialized database for storing and querying vectors.\n",
    "\n",
    "The idea is you:\n",
    "1. Convert documents into vectors.\n",
    "2. Store the vectors in a database.\n",
    "3. Convert queries into vectors.\n",
    "4. Query the database for the closest vectors to the query vector.\n",
    "\n",
    "In practice, this is implemented in the pgvector extension in PostgreSQL. But for this example we will use something called LanceDB, which is just a simple file-system based vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Documents to Vectors\n",
    "The first step to building the vector database is converting the documents into vectors. As we have done in the previous example. But now lets try it with some real documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voluntary = open(\"../voluntary.md\", \"r\").read()\n",
    "\n",
    "embeddings = client.embeddings.create(\n",
    "    input=[glossary, voluntary],\n",
    "    model=\"text-embedding-3-small\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, there was an error here. This model's maximum context length is 8192 tokens, however you requested 38750 tokens. Basically the max number of tokens we can vectorize is 8192 tokens, but we have a bigger document!\n",
    "\n",
    "How do we deal with this? Well here is the second catch. We need to break the documents into smaller chunks.\n",
    "\n",
    "Aside from just getting API to work, this is also import because this way we can have small chunks of the documetns to answer users questions.\n",
    "\n",
    "I kinda soft-sold this part. Its actually probably the hardest part of the RAG system. You need to split the document into smaller chunks, but you probably want to keep similar parts together. So where, and how do you split?\n",
    "\n",
    "There are several strategies to split the document. You could do a recusive split, which is split in the middle, then split again, and again, until everythign fits. But that way you might split on words or sentences. You could try and split on periods, or line breaks, or other characters. Really the options here are endless.\n",
    "\n",
    "For the sake of this, I already split the text into sections and put them in the volutary_sections.json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sections\n",
    "import json\n",
    "\n",
    "with open(\"../voluntary_sections.json\", \"r\") as f:\n",
    "    voluntary_sections = json.load(f)\n",
    "\n",
    "with open(\"../glossary_sections.json\", \"r\") as f:\n",
    "    glossary_sections = json.load(f)\n",
    "\n",
    "\n",
    "all_sections = [section['content'] for section in voluntary_sections] + [section['content'] for section in glossary_sections]\n",
    "\n",
    "all_sections[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now try and vectorize it again\n",
    "embeddings_req = client.embeddings.create(\n",
    "    input=all_sections,\n",
    "    model=\"text-embedding-3-small\",\n",
    ")\n",
    "\n",
    "embeddings = [e.embedding for e in embeddings_req.data]\n",
    "\n",
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "from pydantic import BaseModel\n",
    "from lancedb.pydantic import pydantic_to_schema\n",
    "\n",
    "\n",
    "uri = \"../data/sample-lancedb\"\n",
    "db = lancedb.connect(uri)\n",
    "\n",
    "# create the data, there is a vector and an item for each\n",
    "data = []\n",
    "for i in range(len(all_sections)):\n",
    "    data.append({\"vector\": embeddings[i], \"content\": all_sections[i]})\n",
    "\n",
    "tbl = db.create_table(\"me\", data=data, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the database created, query is as simple as converting the question to a vector and then doing a similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'explain MIQ'\n",
    "\n",
    "question_embedding = client.embeddings.create(\n",
    "    input=[question],\n",
    "    model=\"text-embedding-3-small\",\n",
    ")\n",
    "\n",
    "question_vector = question_embedding.data[0].embedding\n",
    "\n",
    "tbl.search(question_vector).limit(1).to_list()[0]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet. Looks like we got the search engine working. Now lets make a function to handle the whole search process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query: str, results: int = 1) -> list[str]:\n",
    "\n",
    "    question_embedding = client.embeddings.create(\n",
    "        input=[query],\n",
    "        model=\"text-embedding-3-small\",\n",
    "    )\n",
    "\n",
    "    question_vector = question_embedding.data[0].embedding\n",
    "\n",
    "    rows = tbl.search(question_vector).limit(results).to_list()\n",
    "\n",
    "    return [row['content'] for row in rows]\n",
    "\n",
    "search('explain MIQ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put the whole thing together. A simple function that takes a question, searchs for matching documents, then sends those to the LLM to generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(text):\n",
    "\n",
    "    search_results = search(text)\n",
    "\n",
    "    search_results_str = '\\n'.join(search_results)\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "        You are a methane expert, for answering methane related questions. Keep answers short. Make answer markdown.\n",
    "\n",
    "        Use only the provided references to answer the questions.\n",
    "\n",
    "        <References>\n",
    "        {search_results_str}\n",
    "        </References>\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a client object\n",
    "    client = OpenAI()\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": text,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    display(Markdown(completion.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag(\"What is MIQ?\")\n",
    "rag(\"Compare Veritas and MIQ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-overview-ERS0H9D9-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
